# На самом деле, это не совсем парсер.
# Просто высокоуровневая абстракция, которая позволяет более удобно работать с низкоуровневыми библиотеками.
# Например, xpath там вообще как таковой не нужен, а нужная информация в html ищется буквально в пару строчек.

# Импортируем библиотеку, для написания парсеров
from bs4 import BeautifulSoup
# импортируем библиотеку для формирования запросов
import requests


# парсер
def main():
    # пишем ссылку на сайт
    base = 'https://ru.stackoverflow.com'
    # отправляем ГЕТ-запрос и получаем html ресурса
    html = requests.get(base).content
    # получаем объект "супа".
    # Первый аргумент - это весь html
    # Второй аргумент - библиотека для парсинга
    soup = BeautifulSoup(html, 'lxml')
    # с помощью метода "find" находим нужный div, уточняя id
    div = soup.find('div', id='question-mini-list')
    # получаем нужный объект (все теги "а" внутри контейнера "question-mini-list", с классом "s-link")
    # оба атрибута берем из html страницы
    list_a = div.find_all('a', class_='s-link')
    # находим первый объект "а"
    a = div.find('a', class_='s-link')
    parent = a.find_parent()  # Печатаем тег родитель. В данном случае это должен быть тег <h3>
    print(parent)

    # Нередко также бывает, что надо найти родительский тег (т. е. тег, внутри которого находится искомый элемент).
    # Для этого можно воспользоваться методом find_parent(), который работает также, как и find(),
    # но возвращает тег родителя. Слегка поменяем наш код.

    # итерируемся по полученным объектам
    for _ in list_a:
        # выводим текст объекта и абсолютную ссылку
        print(_.getText(), base + _.get('href'))
        print()


if __name__ == "__main__":
    main()